services:

  llm:
      image: svercoutere/mu-python-ml:0.1.0
      environment:
        MODE: "development"
        MU_SPARQL_ENDPOINT: "http://database:8890/sparql"
        MU_SPARQL_UPDATEPOINT: "http://database:8890/sparql"
        MU_QUEUE_GRAPH: "http://mu.semte.ch/application"

        LLM_API_KEY: "PASTE_API_KEY_HERE" # Default value of ollama: ollama 
        LLM_ENDPOINT: "https://api.deepinfra.com/v1/openai" # Can be from any provided that is compatible with the openai (deepinfra, azure, etc.)
        LLM_MODEL_NAME: "meta-llama/Meta-Llama-3.1-8B-Instruct" # Name of the model to use, can be any model the endpoint supports (ollama: you can pull more opensource model from ollama and run them as well locally)
        
      volumes:
      - ./llm:/app/

  ollama: # This is for the local version of the LLM that runs within the stack
    image: ollama/ollama:0.3.11
    ports:
      - 11434:11434
    restart: always
    environment:
      - OLLAMA_HOST=0.0.0.0
    volumes:
      - ./ollama_models:/root/.ollama

      
  identifier:
    image: semtech/mu-identifier:1.10.3
    environment:
      SESSION_COOKIE_SECURE: "on"
    links:
      - dispatcher:dispatcher
 
  dispatcher:
    image: semtech/mu-dispatcher:2.1.0-beta.2
    links:
      - resource:resource
    volumes:
      - ./config/dispatcher:/config
  
  database:
    image: redpencil/virtuoso:1.2.0-rc.1
    environment:
      SPARQL_UPDATE: "true"
      DEFAULT_GRAPH: "http://mu.semte.ch/application"
    volumes:
      - ./data/db:/data
      - ./config/virtuoso/virtuoso.ini:/data/virtuoso.ini
  
  resource:
    image: semtech/mu-cl-resources:1.25.0
    links:
      - database:database
    volumes:
      - ./config/resources:/config
  
  frontend:
    # Skip docker hub and just build it, it's a hackathon...
    build: ../frontend-ai-hackathon-group-4
